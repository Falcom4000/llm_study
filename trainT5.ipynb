{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "662149e0",
   "metadata": {},
   "source": [
    "微调T5模型，使其具备中文问答能力\n",
    "e.g.\n",
    "{\"context\": \"年基准利率4.35%。 从实际看,贷款的基本条件是: 一是中国大陆居民,年龄在60岁以下; 二是有稳定的住址和工作或经营地点; 三是有稳定的收入来源; 四是无不良信用记录,贷款用途不能作为炒股,赌博等行为; 五是具有完全民事行为能力。\", \"answer\": \"年基准利率4.35%\", \"question\": \"2017年银行贷款基准利率\", \"id\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb2a277",
   "metadata": {},
   "source": [
    "# import外部库\n",
    "主要是torch和transformers相关，包括：\n",
    "1. from torch.utils.data import Dataset, DataLoader\n",
    " -- 用于读取数据\n",
    "2. from torch.optim import AdamW -- 优化器\n",
    "3. from transformers import T5Tokenizer, AutoConfig -- 用于分词和读取参数\n",
    "4. from transformers import T5ForConditionalGeneration -- 模型本身\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990e677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import T5Tokenizer, AutoConfig\n",
    "from transformers import T5ForConditionalGeneration\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaca8b0",
   "metadata": {},
   "source": [
    "# 设置随机数\n",
    "确保结果可以复现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8c4dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # some cudnn methods can be random even after fixing the seed\n",
    "    # unless you tell it to be deterministic\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84530c2d",
   "metadata": {},
   "source": [
    "# 构建Dataloader\n",
    "## 构建Dataset\n",
    "继承于pytorch.Dataset，读取train.json和dev.json分别用作训练集和测试集。需要重写__init__ __len__和__getitem__方法\n",
    "## 构建collate_fn\n",
    "输入是字典格式的数据\n",
    "把一个batch内的context和question读入列表。\n",
    "- padding: 填充到相同长度。在attention机制里会通过mask表示哪些是真的哪些是填充的\n",
    "- truncation：截断到最大长度inputs = {\n",
    "    'input_ids': torch.Size([20, 512]),      # (20, 512)\n",
    "    'attention_mask': torch.Size([20, 512])  # (20, 512)\n",
    "}\n",
    "    with tokenizer.as_target_tokenizer():作用是给序列添加上特殊的开始和结束符号\n",
    "\n",
    "        labels.input_ids[labels.input_ids == tokenizer.pad_token_id] = -100\n",
    "将padding部分的值设置为-100 这样就不会参与交叉熵计算\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c171363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5Dataset(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.data = self.load_data(data_file)\n",
    "    \n",
    "    def load_data(self, data_file):\n",
    "        Data = {}\n",
    "        with open(data_file, 'rt') as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                sample = json.loads(line.strip())\n",
    "                Data[idx] = sample\n",
    "        return Data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "def collate_fn(batch_samples):\n",
    "    batch_sentence_1, batch_sentence_2 = [], []\n",
    "    batch_label = []\n",
    "    for sample in batch_samples:\n",
    "        batch_sentence_1.append(sample['context'])\n",
    "        batch_sentence_2.append(sample['question'])\n",
    "        batch_label.append(sample['answer'])\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        [f\"question: {s2} context: {s1}\" for s1, s2 in zip(batch_sentence_1, batch_sentence_2)],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch_label,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "    \n",
    "    # T5ForConditionalGeneration handles the right-shifting of decoder_input_ids internally\n",
    "    # when labels are provided. The parts of labels with padding token (0) will be replaced by -100 to be ignored in loss calculation.\n",
    "    labels.input_ids[labels.input_ids == tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    inputs['labels'] = labels.input_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3942892e",
   "metadata": {},
   "source": [
    "# 设置train\n",
    "- 把input丢入model中，然后把batch转换到cuda上。\n",
    "- outputs = model(**batch)表示按照key value传值\n",
    "- 损失函数用交叉熵，自动忽略padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64024439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, optimizer, lr_scheduler, epoch):\n",
    "    progress_bar = tqdm(range(len(dataloader)))\n",
    "    progress_bar.set_description(f'loss: {0:>7f}')\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0.\n",
    "    for step, batch in enumerate(dataloader, start=1):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_description(f'loss: {epoch_loss/step:>7f}')\n",
    "        progress_bar.update(1)\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adde5f07",
   "metadata": {},
   "source": [
    "# Test loop\n",
    "1. 从测试集中读取数据，放到GPU上\n",
    "2. 执行推理，拿到generated_tokens\n",
    "3. with torch.no_grad() 不计算梯度 labels = batch.pop('labels').to(device) 不保留label\n",
    "4.  model.eval()       # 设置为评估模式（关闭dropout等）\n",
    "5. model.generate 自回归生成 一直生成到结束\n",
    "6. 执行decode，拿到decoded_preds [batch_size, length, vocal_size]\n",
    "7. 把label拿回CPU上，并且还原-100\n",
    "8. 计算BLEU-4和 rouge_L；分别是准确率和召回率\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ab6932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, tokenizer, mode='Valid'):\n",
    "    bleu_metric = evaluate.load(\"bleu\")\n",
    "    rouge_metric = evaluate.load(\"rouge\")\n",
    "    assert mode in ['Valid', 'Test']\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=f\"Evaluating {mode}\"):\n",
    "            labels = batch.pop('labels').to(device)\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            generated_tokens = model.generate(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                max_length=512\n",
    "            ).cpu().numpy()\n",
    "            \n",
    "            decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            \n",
    "            labels = labels.cpu().numpy()\n",
    "            labels[labels == -100] = tokenizer.pad_token_id\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            all_preds.extend(decoded_preds)\n",
    "            all_labels.extend(decoded_labels)\n",
    "\n",
    "    bleu_results = bleu_metric.compute(predictions=all_preds, references=[[l] for l in all_labels])\n",
    "    rouge_results = rouge_metric.compute(predictions=all_preds, references=all_labels)\n",
    "\n",
    "    results = {\n",
    "        \"bleu\": bleu_results['bleu'],\n",
    "        \"rougeL\": rouge_results['rougeL']\n",
    "    }\n",
    "    \n",
    "    print(f\"{mode} BLEU-4: {results['bleu']:.4f}\")\n",
    "    print(f\"{mode} Rouge-L: {results['rougeL']:.4f}\\n\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66a1e89",
   "metadata": {},
   "source": [
    "# 训练\n",
    "1. 20个epoch\n",
    "2. 保留bleu-4最大的权重\n",
    "3. 保留每个epoch的loss和bleu-4 rogel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa8c9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "seed_everything(42)\n",
    "\n",
    "learning_rate = 1e-5\n",
    "batch_size = 20\n",
    "epoch_num = 20\n",
    "\n",
    "checkpoint = \"./model\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "train_data = T5Dataset('train.json')\n",
    "valid_data = T5Dataset('dev.json')\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dataloader= DataLoader(valid_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "model = T5ForConditionalGeneration.from_pretrained(checkpoint, config=config).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=epoch_num*len(train_dataloader),\n",
    ")\n",
    "\n",
    "all_losses = []\n",
    "all_valid_metrics = []\n",
    "best_rouge = 0.\n",
    "for t in range(epoch_num):\n",
    "    print(f\"Epoch {t+1}/{epoch_num}\\n-------------------------------\")\n",
    "    epoch_loss = train_loop(train_dataloader, model, optimizer, lr_scheduler, t+1)\n",
    "    all_losses.append(epoch_loss)\n",
    "    \n",
    "    # Save losses after each epoch\n",
    "    with open('train_losses.json', 'w') as f:\n",
    "        json.dump(all_losses, f, indent=4)\n",
    "    \n",
    "    valid_metrics = test_loop(valid_dataloader, model, tokenizer, mode='Valid')\n",
    "    all_valid_metrics.append(valid_metrics)\n",
    "    with open('valid_metrics.json', 'w') as f:\n",
    "        json.dump(all_valid_metrics, f, indent=4)\n",
    "\n",
    "    if valid_metrics['rougeL'] > best_rouge:\n",
    "        best_rouge = valid_metrics['rougeL']\n",
    "        print('saving new best weights...\\n')\n",
    "        torch.save(model.state_dict(), f'epoch_{t+1}_rougeL_{best_rouge:.4f}_model_weights.bin')\n",
    "    \n",
    "    print('saving current epoch weights...\\n')\n",
    "    torch.save(model.state_dict(), f'epoch_{t+1}_model_weights.bin')\n",
    "\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
